srun: ROUTE: split_hostlist: hl=bun005 tree_width 0
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_173202-wk8qvf25
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_finger_spin_run_seed_1
wandb: â­ï¸ View project at https://wandb.ai/rodxiang2/visualRL
wandb: ğŸš€ View run at https://wandb.ai/rodxiang2/visualRL/runs/wk8qvf25
libEGL warning: egl: failed to create dri2 screen
wandb: WARNING Unhandled define_metric() arg: stetp_metric
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
wandb: Currently logged in as: rodxiang2. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_173221-a43sn6ar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_cheetah_run_run_seed_1
wandb: â­ï¸ View project at https://wandb.ai/rodxiang2/visualRL
wandb: ğŸš€ View run at https://wandb.ai/rodxiang2/visualRL/runs/a43sn6ar
wandb: WARNING Unhandled define_metric() arg: stetp_metric
Error executing job with overrides: ['agent=taco', 'task=finger_spin', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [2] at entry 0 and [6] at entry 819


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.041 MB uploadedwandb: | 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode â–
wandb:   eval/episode_length â–
wandb:   eval/episode_reward â–
wandb:             eval/step â–
wandb:       eval_total_time â–
wandb:           global_step â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/actor_ent â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–
wandb:   train/actor_logprob â–ˆâ–‡â–â–‡â–‡â–ˆâ–‡â–‚
wandb:      train/actor_loss â–ˆâ–„â–„â–‚â–ƒâ–ƒâ–â–
wandb:    train/batch_reward â–„â–…â–â–ƒâ–ƒâ–…â–ˆâ–ƒ
wandb:     train/critic_loss â–†â–‡â–â–ƒâ–ƒâ–‡â–ˆâ–„
wandb:       train/critic_q1 â–ˆâ–â–†â–†â–…â–„â–…â–†
wandb:       train/critic_q2 â–â–ˆâ–â–â–„â–†â–‡â–‡
wandb: train/critic_target_q â–‚â–ƒâ–â–ƒâ–„â–†â–ˆâ–…
wandb:       train/curl_loss â–ˆâ–ƒâ–â–â–â–â–â–
wandb:     train/reward_loss â–†â–‡â–â–ƒâ–ƒâ–‡â–ˆâ–„
wandb:       train/taco_loss â–ˆâ–…â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:          eval/episode 0
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 0.0
wandb:             eval/step 0
wandb:       eval_total_time 0.02061
wandb:           global_step 4028
wandb:       train/actor_ent 2.79204
wandb:   train/actor_logprob -1.87211
wandb:      train/actor_loss 0.03639
wandb:    train/batch_reward 0.01641
wandb:     train/critic_loss 0.13529
wandb:       train/critic_q1 -0.03839
wandb:       train/critic_q2 -0.01178
wandb: train/critic_target_q -0.03952
wandb:       train/curl_loss 6.94807
wandb:     train/reward_loss 0.06797
wandb:       train/taco_loss 6.93095
wandb: 
wandb: ğŸš€ View run TACO_finger_spin_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/wk8qvf25
wandb: â­ï¸ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_173202-wk8qvf25/logs
srun: error: bun005: task 0: Exited with exit code 1
Error executing job with overrides: ['agent=taco', 'task=cheetah_run', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [6] at entry 0 and [1] at entry 986


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.034 MB of 0.043 MB uploadedwandb: | 0.043 MB of 0.043 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode â–â–…â–ˆ
wandb:   eval/episode_length â–â–â–
wandb:   eval/episode_reward â–â–†â–ˆ
wandb:             eval/step â–â–…â–ˆ
wandb:       eval_total_time â–â–„â–ˆ
wandb:           global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       train/actor_ent â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:   train/actor_logprob â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      train/actor_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–
wandb:    train/batch_reward â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡
wandb:     train/buffer_size â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:     train/critic_loss â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–„â–…â–„â–†â–…â–†â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–†
wandb:       train/critic_q1 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/critic_q2 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/critic_target_q â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/curl_loss â–ˆâ–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train/episode â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:  train/episode_length â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/episode_reward â–â–‚â–‚â–„â–â–ƒâ–…â–…â–ƒâ–…â–‡â–„â–‡â–†â–„â–ˆ
wandb:             train/fps â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/reward_loss â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–†â–…â–†â–„â–„â–…â–„â–‡â–†â–…â–†â–…â–‡â–‡â–†â–†â–…â–‡â–‡â–ˆâ–†â–†â–…â–…â–‡â–„â–…â–„â–„â–„
wandb:            train/step â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:       train/taco_loss â–ˆâ–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      train/total_time â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:          eval/episode 20
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 133.62684
wandb:             eval/step 10000
wandb:       eval_total_time 2765.45746
wandb:           global_step 20336
wandb:       train/actor_ent 7.78476
wandb:   train/actor_logprob -4.92753
wandb:      train/actor_loss -12.32044
wandb:    train/batch_reward 0.35747
wandb:     train/buffer_size 10000
wandb:     train/critic_loss 0.63024
wandb:       train/critic_q1 11.95378
wandb:       train/critic_q2 11.97372
wandb: train/critic_target_q 11.97165
wandb:       train/curl_loss 0.24464
wandb:         train/episode 20
wandb:  train/episode_length 1000
wandb:  train/episode_reward 155.09492
wandb:             train/fps 6.36334
wandb:     train/reward_loss 0.01696
wandb:            train/step 10000
wandb:       train/taco_loss 0.36905
wandb:      train/total_time 2765.42995
wandb: 
wandb: ğŸš€ View run TACO_cheetah_run_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/a43sn6ar
wandb: â­ï¸ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_173221-a43sn6ar/logs
srun: error: bun005: task 0: Exited with exit code 1
