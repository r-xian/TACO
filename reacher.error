srun: ROUTE: split_hostlist: hl=bun005 tree_width 0
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_173202-wk8qvf25
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_finger_spin_run_seed_1
wandb: ⭐️ View project at https://wandb.ai/rodxiang2/visualRL
wandb: 🚀 View run at https://wandb.ai/rodxiang2/visualRL/runs/wk8qvf25
libEGL warning: egl: failed to create dri2 screen
wandb: WARNING Unhandled define_metric() arg: stetp_metric
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
wandb: Currently logged in as: rodxiang2. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_173221-a43sn6ar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_cheetah_run_run_seed_1
wandb: ⭐️ View project at https://wandb.ai/rodxiang2/visualRL
wandb: 🚀 View run at https://wandb.ai/rodxiang2/visualRL/runs/a43sn6ar
wandb: WARNING Unhandled define_metric() arg: stetp_metric
Error executing job with overrides: ['agent=taco', 'task=finger_spin', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [2] at entry 0 and [6] at entry 819


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.041 MB uploadedwandb: | 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode ▁
wandb:   eval/episode_length ▁
wandb:   eval/episode_reward ▁
wandb:             eval/step ▁
wandb:       eval_total_time ▁
wandb:           global_step ▁▁▁█████████████████████████████████████
wandb:       train/actor_ent █▇▆▅▄▃▂▁
wandb:   train/actor_logprob █▇▁▇▇█▇▂
wandb:      train/actor_loss █▄▄▂▃▃▁▁
wandb:    train/batch_reward ▄▅▁▃▃▅█▃
wandb:     train/critic_loss ▆▇▁▃▃▇█▄
wandb:       train/critic_q1 █▁▆▆▅▄▅▆
wandb:       train/critic_q2 ▁█▁▁▄▆▇▇
wandb: train/critic_target_q ▂▃▁▃▄▆█▅
wandb:       train/curl_loss █▃▁▁▁▁▁▁
wandb:     train/reward_loss ▆▇▁▃▃▇█▄
wandb:       train/taco_loss █▅▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:          eval/episode 0
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 0.0
wandb:             eval/step 0
wandb:       eval_total_time 0.02061
wandb:           global_step 4028
wandb:       train/actor_ent 2.79204
wandb:   train/actor_logprob -1.87211
wandb:      train/actor_loss 0.03639
wandb:    train/batch_reward 0.01641
wandb:     train/critic_loss 0.13529
wandb:       train/critic_q1 -0.03839
wandb:       train/critic_q2 -0.01178
wandb: train/critic_target_q -0.03952
wandb:       train/curl_loss 6.94807
wandb:     train/reward_loss 0.06797
wandb:       train/taco_loss 6.93095
wandb: 
wandb: 🚀 View run TACO_finger_spin_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/wk8qvf25
wandb: ⭐️ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_173202-wk8qvf25/logs
srun: error: bun005: task 0: Exited with exit code 1
Error executing job with overrides: ['agent=taco', 'task=cheetah_run', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [6] at entry 0 and [1] at entry 986


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.034 MB of 0.043 MB uploadedwandb: | 0.043 MB of 0.043 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode ▁▅█
wandb:   eval/episode_length ▁▁▁
wandb:   eval/episode_reward ▁▆█
wandb:             eval/step ▁▅█
wandb:       eval_total_time ▁▄█
wandb:           global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:       train/actor_ent ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:   train/actor_logprob ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:      train/actor_loss ████████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁
wandb:    train/batch_reward ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▅▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███▇
wandb:     train/buffer_size ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:     train/critic_loss ▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▄▃▃▄▃▄▄▄▄▄▅▄▆▅▆▆▆▇█▇▇▇▆
wandb:       train/critic_q1 ▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇████
wandb:       train/critic_q2 ▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇████
wandb: train/critic_target_q ▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇████
wandb:       train/curl_loss █▇▅▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/episode ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:  train/episode_length ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  train/episode_reward ▁▂▂▄▁▃▅▅▃▅▇▄▇▆▄█
wandb:             train/fps ▁█████▃█████████
wandb:     train/reward_loss ▁▁▁▂▂▃▂▂▃▃▆▅▆▄▄▅▄▇▆▅▆▅▇▇▆▆▅▇▇█▆▆▅▅▇▄▅▄▄▄
wandb:            train/step ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:       train/taco_loss ██▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train/total_time ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb: 
wandb: Run summary:
wandb:          eval/episode 20
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 133.62684
wandb:             eval/step 10000
wandb:       eval_total_time 2765.45746
wandb:           global_step 20336
wandb:       train/actor_ent 7.78476
wandb:   train/actor_logprob -4.92753
wandb:      train/actor_loss -12.32044
wandb:    train/batch_reward 0.35747
wandb:     train/buffer_size 10000
wandb:     train/critic_loss 0.63024
wandb:       train/critic_q1 11.95378
wandb:       train/critic_q2 11.97372
wandb: train/critic_target_q 11.97165
wandb:       train/curl_loss 0.24464
wandb:         train/episode 20
wandb:  train/episode_length 1000
wandb:  train/episode_reward 155.09492
wandb:             train/fps 6.36334
wandb:     train/reward_loss 0.01696
wandb:            train/step 10000
wandb:       train/taco_loss 0.36905
wandb:      train/total_time 2765.42995
wandb: 
wandb: 🚀 View run TACO_cheetah_run_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/a43sn6ar
wandb: ⭐️ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_173221-a43sn6ar/logs
srun: error: bun005: task 0: Exited with exit code 1
