srun: ROUTE: split_hostlist: hl=bun005 tree_width 0
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
wandb: Currently logged in as: rodxiang2. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_183236-q9qpg2tu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_walker_walk_run_seed_1
wandb: â­ï¸ View project at https://wandb.ai/rodxiang2/visualRL
wandb: ğŸš€ View run at https://wandb.ai/rodxiang2/visualRL/runs/q9qpg2tu
Error executing job with overrides: ['agent=taco', 'task=walker_walk', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [6] at entry 0 and [2] at entry 515


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.034 MB of 0.041 MB uploadedwandb: | 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode â–
wandb:   eval/episode_length â–
wandb:   eval/episode_reward â–
wandb:             eval/step â–
wandb:       eval_total_time â–
wandb:       train/actor_ent â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:   train/actor_logprob â–â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      train/actor_loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:    train/batch_reward â–†â–‡â–‡â–ˆâ–ˆâ–†â–‡â–…â–…â–„â–…â–…â–…â–…â–„â–ƒâ–…â–…â–…â–…â–…â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–„â–‚â–‚â–‚
wandb:     train/buffer_size â–â–ƒâ–…â–†â–ˆ
wandb:     train/critic_loss â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–ˆâ–…â–„â–ƒâ–ƒ
wandb:       train/critic_q1 â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/critic_q2 â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/critic_target_q â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       train/curl_loss â–ˆâ–ˆâ–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train/episode â–â–ƒâ–…â–†â–ˆ
wandb:  train/episode_length â–â–â–â–â–
wandb:  train/episode_reward â–„â–‡â–‚â–â–ˆ
wandb:             train/fps â–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train/reward_loss â–‡â–†â–ˆâ–‡â–‡â–†â–‡â–…â–…â–„â–…â–„â–„â–„â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–ƒâ–‚â–‚â–â–
wandb:            train/step â–â–ƒâ–…â–†â–ˆ
wandb:       train/taco_loss â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–
wandb:      train/total_time â–â–ƒâ–…â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:          eval/episode 0
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 20.59911
wandb:             eval/step 0
wandb:       eval_total_time 0.02617
wandb:       train/actor_ent 8.17542
wandb:   train/actor_logprob -5.29905
wandb:      train/actor_loss -1.11176
wandb:    train/batch_reward 0.06448
wandb:     train/buffer_size 4500
wandb:     train/critic_loss 0.00768
wandb:       train/critic_q1 1.11062
wandb:       train/critic_q2 1.11009
wandb: train/critic_target_q 1.11379
wandb:       train/curl_loss 1.06265
wandb:         train/episode 9
wandb:  train/episode_length 1000
wandb:  train/episode_reward 38.36568
wandb:             train/fps 6.336
wandb:     train/reward_loss 0.00123
wandb:            train/step 4500
wandb:       train/taco_loss 2.00279
wandb:      train/total_time 942.09265
wandb: 
wandb: ğŸš€ View run TACO_walker_walk_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/q9qpg2tu
wandb: â­ï¸ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_183236-q9qpg2tu/logs
srun: error: bun005: task 0: Exited with exit code 1
