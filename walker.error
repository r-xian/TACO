srun: ROUTE: split_hostlist: hl=bun005 tree_width 0
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
libEGL warning: egl: failed to create dri2 screen
wandb: Currently logged in as: rodxiang2. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/s4642506/TACO/exp_local/default/wandb/run-20240520_183236-q9qpg2tu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TACO_walker_walk_run_seed_1
wandb: ⭐️ View project at https://wandb.ai/rodxiang2/visualRL
wandb: 🚀 View run at https://wandb.ai/rodxiang2/visualRL/runs/q9qpg2tu
Error executing job with overrides: ['agent=taco', 'task=walker_walk', 'seed=1']
Traceback (most recent call last):
  File "./train.py", line 211, in main
    workspace.train()
  File "/home/s4642506/TACO/train.py", line 176, in train
    metrics = self.agent.update(self.replay_iter, self.global_step)
  File "/home/s4642506/TACO/agents/taco.py", line 356, in update
    batch = next(replay_iter)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/_utils.py", line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 40, in fetch
    return self.collate_fn(data)
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 84, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 64, in default_collate
    return default_collate([torch.as_tensor(b) for b in batch])
  File "/home/s4642506/.conda/envs/taco/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 56, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [6] at entry 0 and [2] at entry 515


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.034 MB of 0.041 MB uploadedwandb: | 0.041 MB of 0.041 MB uploadedwandb: 
wandb: Run history:
wandb:          eval/episode ▁
wandb:   eval/episode_length ▁
wandb:   eval/episode_reward ▁
wandb:             eval/step ▁
wandb:       eval_total_time ▁
wandb:       train/actor_ent ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:   train/actor_logprob ▁▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████
wandb:      train/actor_loss ████▇▇▇▆▆▆▆▆▆▆▆▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁
wandb:    train/batch_reward ▆▇▇██▆▇▅▅▄▅▅▅▅▄▃▅▅▅▅▅▂▂▃▃▃▃▄▁▂▂▃▁▂▂▂▄▂▂▂
wandb:     train/buffer_size ▁▃▅▆█
wandb:     train/critic_loss ▅▅▄▃▃▂▂▂▂▂▁▁▁▁▂▂▂▂▂▃▃▄▃▃▃▃▄▄▃▃▄▄▃▄▄█▅▄▃▃
wandb:       train/critic_q1 ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████
wandb:       train/critic_q2 ▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████
wandb: train/critic_target_q ▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:       train/curl_loss ██▆▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train/episode ▁▃▅▆█
wandb:  train/episode_length ▁▁▁▁▁
wandb:  train/episode_reward ▄▇▂▁█
wandb:             train/fps ▁████
wandb:     train/reward_loss ▇▆█▇▇▆▇▅▅▄▅▄▄▄▃▃▄▄▃▃▂▃▂▂▂▂▂▂▁▂▁▂▁▂▁▃▂▂▁▁
wandb:            train/step ▁▃▅▆█
wandb:       train/taco_loss ██▇▆▅▅▄▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁
wandb:      train/total_time ▁▃▅▆█
wandb: 
wandb: Run summary:
wandb:          eval/episode 0
wandb:   eval/episode_length 1000.0
wandb:   eval/episode_reward 20.59911
wandb:             eval/step 0
wandb:       eval_total_time 0.02617
wandb:       train/actor_ent 8.17542
wandb:   train/actor_logprob -5.29905
wandb:      train/actor_loss -1.11176
wandb:    train/batch_reward 0.06448
wandb:     train/buffer_size 4500
wandb:     train/critic_loss 0.00768
wandb:       train/critic_q1 1.11062
wandb:       train/critic_q2 1.11009
wandb: train/critic_target_q 1.11379
wandb:       train/curl_loss 1.06265
wandb:         train/episode 9
wandb:  train/episode_length 1000
wandb:  train/episode_reward 38.36568
wandb:             train/fps 6.336
wandb:     train/reward_loss 0.00123
wandb:            train/step 4500
wandb:       train/taco_loss 2.00279
wandb:      train/total_time 942.09265
wandb: 
wandb: 🚀 View run TACO_walker_walk_run_seed_1 at: https://wandb.ai/rodxiang2/visualRL/runs/q9qpg2tu
wandb: ⭐️ View project at: https://wandb.ai/rodxiang2/visualRL
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240520_183236-q9qpg2tu/logs
srun: error: bun005: task 0: Exited with exit code 1
